# Multimodal Deepfake Detection - Quick Reference

## ‚ö° Quick Start (Copy & Paste)

### 1. Test Your Setup
```bash
cd e:\project\aura-veracity-lab\model-service
python test_multimodal_setup.py
```

### 2. Extract Audio
```bash
python scripts/extract_audio_multimodal.py \
    --video-dir ../FaceForensics-master/original_sequences/youtube/raw/videos \
    --output-dir data/processed/audio \
    --label real
```

### 3. Use in Training
```python
from src.datasets.multimodal_dataset import create_multimodal_dataloaders

loaders = create_multimodal_dataloaders(
    data_dir='data/processed',
    batch_size=32,
    audio_feature='spectrogram'
)

for batch in loaders['train']:
    frames = batch['frames']      # [32, 10, 3, 224, 224]
    audio = batch['audio']        # [32, 80, 300]
    labels = batch['label']       # [32]
    
    outputs = model(video=frames, audio=audio)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
```

---

## üìä What You Have

| Component | Status | Location |
|-----------|--------|----------|
| Video frames | ‚úÖ Ready | `data/processed/{train,val,test}/{fake,real}/` |
| Raw videos | ‚úÖ Ready | `FaceForensics-master/original_sequences/youtube/raw/videos/` |
| Audio extraction | ‚úÖ Ready | `scripts/extract_audio_multimodal.py` |
| Audio processor | ‚úÖ Ready | `src/preprocessing/audio_processor.py` |
| Multimodal loader | ‚úÖ Ready | `src/datasets/multimodal_dataset.py` |
| Alignment verifier | ‚úÖ Ready | `scripts/verify_multimodal_alignment.py` |

---

## üéØ Expected Results

```
Video Only:         AUC=0.93, Accuracy=89%
Multimodal:         AUC=0.96, Accuracy=94%
Improvement:        +3-5% AUC, +5% Accuracy
```

---

## üîß Common Commands

### Extract Audio from Real Videos
```bash
python scripts/extract_audio_multimodal.py \
    --video-dir ../FaceForensics-master/original_sequences/youtube/raw/videos \
    --output-dir data/processed/audio \
    --label real
```

### Extract Audio from Fake Videos
```bash
python scripts/extract_audio_multimodal.py \
    --video-dir data/FaceForensics++/manipulated_sequences/Deepfakes/c40/videos \
    --output-dir data/processed/audio \
    --label fake \
    --max-videos 100  # Test with subset first
```

### Verify Alignment
```bash
# Check single split
python scripts/verify_multimodal_alignment.py \
    --frame-dir data/processed/train/fake \
    --audio-dir data/processed/audio/train/fake

# Check all splits
python scripts/verify_multimodal_alignment.py \
    --data-dir data/processed \
    --all-splits

# Save report
python scripts/verify_multimodal_alignment.py \
    --data-dir data/processed \
    --all-splits \
    --output-report alignment_report.json
```

### Test Audio Processing
```python
from src.preprocessing.audio_processor import AudioProcessor

processor = AudioProcessor(sample_rate=16000, n_mels=80)

# Spectrogram
spec = processor.audio_to_spectrogram('audio.wav')
print(f"Spectrogram shape: {spec.shape}")  # [80, ~300]

# MFCC
mfcc = processor.audio_to_mfcc('audio.wav')
print(f"MFCC shape: {mfcc.shape}")  # [13, ~300]

# Waveform (for Wav2Vec2)
waveform = processor.audio_to_waveform('audio.wav')
print(f"Waveform shape: {waveform.shape}")  # [48000]
```

### Test DataLoader
```python
from src.datasets.multimodal_dataset import MultimodalDeepfakeDataset, verify_multimodal_batch

dataset = MultimodalDeepfakeDataset(
    frame_dir='data/processed/train/fake',
    audio_dir='data/processed/audio/train/fake',
    audio_feature='spectrogram',
    frames_per_video=10
)

sample = dataset[0]
print(f"Frames: {sample['frames'].shape}")
print(f"Audio: {sample['audio'].shape}")
print(f"Label: {sample['label']}")

# Batch verification
from torch.utils.data import DataLoader
loader = DataLoader(dataset, batch_size=32)
batch = next(iter(loader))
verify_multimodal_batch(batch)
```

---

## üìÅ File Locations

```
model-service/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ extract_audio_multimodal.py          (450 lines)
‚îÇ   ‚îî‚îÄ‚îÄ verify_multimodal_alignment.py       (480 lines)
‚îú‚îÄ‚îÄ src/preprocessing/
‚îÇ   ‚îî‚îÄ‚îÄ audio_processor.py                   (400 lines)
‚îú‚îÄ‚îÄ src/datasets/
‚îÇ   ‚îî‚îÄ‚îÄ multimodal_dataset.py                (550 lines)
‚îú‚îÄ‚îÄ MULTIMODAL_SETUP.md                      (400 lines) ‚Üê Start here!
‚îú‚îÄ‚îÄ MULTIMODAL_COMPLETE_SETUP.md             (350 lines)
‚îú‚îÄ‚îÄ ARCHITECTURE_DIAGRAM.md                  (400 lines)
‚îî‚îÄ‚îÄ test_multimodal_setup.py                 (300 lines)
```

---

## üöÄ Next Steps

### Today
- [ ] Run `python test_multimodal_setup.py`
- [ ] Extract audio from 43 real videos
- [ ] Verify alignment

### This Week
- [ ] Test training with multimodal loader
- [ ] Compare results: video-only vs multimodal
- [ ] Run ablation study

### Next Week
- [ ] Download DFDC dataset (optional, for higher accuracy)
- [ ] Scale training to full dataset
- [ ] Generate publication plots

---

## üí° Pro Tips

### For Faster Training
```python
# Use fewer frames
dataset = MultimodalDeepfakeDataset(
    ...,
    frames_per_video=5  # Default: 10
)

# Use MFCC instead of spectrogram (smaller)
loaders = create_multimodal_dataloaders(
    ...,
    audio_feature='mfcc'  # Instead of 'spectrogram'
)

# Reduce batch size if memory limited
loader = DataLoader(dataset, batch_size=16)
```

### For Better Accuracy
```python
# Use more frames
frames_per_video=20

# Use full spectrogram (80 mel bins)
n_mels=80

# Process longer audio clips
audio_duration=5.0  # seconds

# Enable audio augmentation
augment_audio=True
```

### For GPU Acceleration
```python
# Use pin_memory and workers
loader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=8,
    pin_memory=True
)

# Move tensors to GPU
frames = batch['frames'].cuda()
audio = batch['audio'].cuda()
```

---

## üìà Monitoring Performance

### Check Data Quality
```bash
python scripts/verify_multimodal_alignment.py --data-dir data/processed --all-splits
```

### Check Loading Speed
```python
import time
from torch.utils.data import DataLoader
from src.datasets.multimodal_dataset import MultimodalDeepfakeDataset

dataset = MultimodalDeepfakeDataset(...)
loader = DataLoader(dataset, batch_size=32, num_workers=4)

start = time.time()
for batch in loader:
    pass
elapsed = time.time() - start
print(f"Time to load {len(loader)} batches: {elapsed:.2f}s")
print(f"Throughput: {len(dataset) / elapsed:.0f} samples/sec")
```

---

## üêõ Troubleshooting

### Problem: "FFmpeg not found"
**Solution:** Install FFmpeg
```bash
pip install ffmpeg-python
# Or: conda install ffmpeg
```

### Problem: "No audio files found"
**Solution:** Run extraction first
```bash
python scripts/extract_audio_multimodal.py --video-dir ... --output-dir data/processed/audio
```

### Problem: "Memory error"
**Solution:** Reduce batch size or frames per video
```python
loaders = create_multimodal_dataloaders(
    data_dir='data/processed',
    batch_size=8,      # Reduce from 32
    frames_per_video=5 # Reduce from 10
)
```

### Problem: "DataLoader is slow"
**Solution:** Increase workers
```python
loader = DataLoader(dataset, num_workers=8, pin_memory=True)
```

---

## üìö Documentation Map

| Document | Purpose | When to Read |
|----------|---------|--------------|
| QUICK_REFERENCE.md | This file - commands | Before starting |
| MULTIMODAL_SETUP.md | Detailed instructions | Step-by-step guide |
| MULTIMODAL_COMPLETE_SETUP.md | Full documentation | Reference during setup |
| ARCHITECTURE_DIAGRAM.md | Visual architecture | Understand data flow |
| Scripts docstrings | Code documentation | Debug specific scripts |

---

## üìù Code Examples

### Complete Training Loop
```python
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from src.datasets.multimodal_dataset import create_multimodal_dataloaders
from src.models import YourMultimodalModel

# Setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = YourMultimodalModel(enable_video=True, enable_audio=True).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

# Load data
loaders = create_multimodal_dataloaders(
    data_dir='data/processed',
    batch_size=32,
    audio_feature='spectrogram'
)

# Training
for epoch in range(10):
    model.train()
    for batch in loaders['train']:
        frames = batch['frames'].to(device)
        audio = batch['audio'].to(device)
        labels = batch['label'].to(device)
        
        outputs = model(video=frames, audio=audio)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # Validation
    model.eval()
    with torch.no_grad():
        val_loss = 0
        for batch in loaders['val']:
            frames = batch['frames'].to(device)
            audio = batch['audio'].to(device)
            labels = batch['label'].to(device)
            
            outputs = model(video=frames, audio=audio)
            val_loss += criterion(outputs, labels).item()
    
    print(f"Epoch {epoch+1}: val_loss={val_loss/len(loaders['val']):.4f}")
```

---

## üéì Learning Path

1. **Understand the data**: Read ARCHITECTURE_DIAGRAM.md
2. **Setup components**: Follow MULTIMODAL_SETUP.md
3. **Extract audio**: Run `python scripts/extract_audio_multimodal.py`
4. **Verify alignment**: Run `python scripts/verify_multimodal_alignment.py`
5. **Load data**: Use `create_multimodal_dataloaders()`
6. **Train model**: Use the training loop example
7. **Publish results**: Write methodology section

---

## ‚úÖ Checklist Before Training

- [ ] Run `test_multimodal_setup.py` successfully
- [ ] Audio files extracted to `data/processed/audio/`
- [ ] Alignment verification passes
- [ ] Can load batch from DataLoader
- [ ] Model accepts (video, audio) input
- [ ] GPU available (optional but recommended)
- [ ] Enough disk space (~100 GB if scaling to DFDC)

---

## üìû Support

For issues, check:
1. Script docstrings: `python scripts/extract_audio_multimodal.py --help`
2. Module docs: Open Python files and read docstrings
3. MULTIMODAL_SETUP.md for detailed steps
4. Python errors: They're usually very descriptive!

---

**Ready to train?** üöÄ

```bash
python test_multimodal_setup.py
python scripts/extract_audio_multimodal.py --video-dir ../FaceForensics-master/original_sequences/youtube/raw/videos --output-dir data/processed/audio --label real
```

Good luck! üéâ
