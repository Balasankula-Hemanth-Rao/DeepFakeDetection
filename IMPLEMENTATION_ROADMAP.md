# **IMPLEMENTATION ROADMAP ‚Äî MULTIMODAL DEEPFAKE DETECTION v1.0**

**Locked Model Contract:** See [ML_SYSTEM_DESIGN.md ¬ß XI](ML_SYSTEM_DESIGN.md#xi-model-contract-v1-locked-decisions-)

**Current Date:** January 3, 2026  
**Target Completion:** Q1 2026 (12 weeks)

---

## **üìã PHASE 1: CRITICAL FIXES (Weeks 1‚Äì2)**

### **Priority 1.1: Audio Encoder Replacement** ‚≠ê HIGHEST IMPACT
**Expected Gain:** +5‚Äì10% AUC  
**Effort:** 3‚Äì4 days

**Tasks:**
- [ ] Remove naive AudioCNN from `models/audio_cnn.py` (or deprecate)
- [ ] Create `models/audio_encoder.py` with wav2vec2 wrapper
  - Load facebook/wav2vec2-base pretrained weights
  - Feature dimension: 768 (output of final layer)
  - Freeze first 8 layers, fine-tune last 4 + linear projection to 512
- [ ] Update `data/multimodal_dataset.py`:
  - Load `.wav` files at 16kHz, mono (use librosa)
  - NO mel-spectrogram extraction (wav2vec2 expects raw waveform)
  - Update augmentation: pitch shift, time stretch (keep non-destructive)
- [ ] Update `models/multimodal_model.py`:
  - Replace `AudioCNN` input with raw waveform (variable length)
  - Implement `AudioFeatureExtractor` with wav2vec2
  - Update fusion input dimensions: video (2048 after temporal) + audio (512)
- [ ] Update `train.py`:
  - Pass raw waveforms instead of mel-specs
  - Adjust batch collation for variable-length audio
- [ ] Verify: Load FaceForensics++ sample, confirm audio features extracted correctly

**Acceptance Criteria:**
- ‚úÖ wav2vec2 features extracted successfully
- ‚úÖ Model forward pass completes without error
- ‚úÖ Training loop runs (1 epoch) without OOM
- ‚úÖ Audio feature shape: (batch, seq_len, 512)

---

### **Priority 1.2: Voice Activity Detection (VAD)**
**Expected Gain:** +1‚Äì2% AUC  
**Effort:** 2‚Äì3 days

**Tasks:**
- [ ] Create `preprocess/voice_activity_detection.py`
  - Option A: pyannote-audio (better but requires setup)
  - Option B: librosa energy-based VAD (simpler, good enough for v1)
  - Output: timestamps of speech regions
- [ ] Integrate into `data/multimodal_dataset.py`:
  - Apply VAD to extracted audio
  - Mask non-speech regions with zeros (or remove)
  - Update mel-spec extraction to respect VAD mask
- [ ] Update `multimodal_model.py`:
  - Accept optional VAD mask in forward pass
  - Zero out audio features for non-speech frames
- [ ] Update `train.py`:
  - Load VAD masks during training
  - Add metric: % of silence removed

**Acceptance Criteria:**
- ‚úÖ VAD extracts speech regions correctly
- ‚úÖ Silence is masked in audio features
- ‚úÖ Training AUC improves by 1‚Äì2%

---

### **Priority 1.3: Integrate Temporal Consistency Loss**
**Expected Gain:** +2‚Äì3% AUC  
**Effort:** 1‚Äì2 days

**Tasks:**
- [ ] Review `models/losses.py` (already defined; just not used)
- [ ] Update `train.py`:
  - Uncomment `TemporalConsistencyLoss` import
  - Add to loss computation with weight Œª=0.1 (tune later)
  - Total loss = cross_entropy(pred, label) + Œª √ó temporal_consistency(embeddings)
- [ ] Test training with new loss:
  - Verify loss decreases
  - Check that embeddings become more stable frame-to-frame
- [ ] Tune weight Œª on validation set
  - Sweep Œª ‚àà {0.01, 0.05, 0.1, 0.2}
  - Choose Œª that maximizes validation AUC

**Acceptance Criteria:**
- ‚úÖ Loss function integrated into training loop
- ‚úÖ Model trains without error
- ‚úÖ Validation AUC improves by 2‚Äì3%

---

### **Priority 1.4: Video-Level Inference Endpoint**
**Expected Gain:** Architectural necessity (0% AUC, 100% UX)  
**Effort:** 2‚Äì3 days

**Tasks:**
- [ ] Create `serve/api_video.py` with FastAPI routes:
  - POST `/analyze-video` ‚Üí returns `{"job_id": "uuid", "status_url": "/jobs/{job_id}"}`
  - GET `/jobs/{job_id}` ‚Üí returns current status + results (if complete)
- [ ] Implement video processing pipeline in `serve/inference.py`:
  - Extract frames at 5 FPS using FFmpeg
  - Run FrameModel on each frame (batch inference)
  - Aggregate frame predictions: mean confidence + saliency
  - Generate output JSON (see ML_SYSTEM_DESIGN.md ¬ß D)
- [ ] Update backend to call model service:
  - Backend POST `/uploads/init-job` ‚Üí enqueue to Celery
  - Celery worker: calls model service `/infer-video`
  - Store results in PostgreSQL `detection_results` table
- [ ] Test end-to-end:
  - Upload video via frontend
  - Monitor job status
  - Retrieve results + saliency maps

**Acceptance Criteria:**
- ‚úÖ `/analyze-video` endpoint accepts video file
- ‚úÖ Video processed, frames extracted, model inference runs
- ‚úÖ Results aggregated and stored
- ‚úÖ Frontend can retrieve + display results

---

### **Priority 1.5: Fix Modality Dropout**
**Expected Gain:** +0.5‚Äì1% AUC (regularization)  
**Effort:** 1 day

**Tasks:**
- [ ] Review `models/multimodal_model.py` (config parameter exists but not used)
- [ ] Update forward pass:
  - During training: drop audio OR video features with probability `modality_dropout_prob`
  - During inference: disable dropout (always use both modalities)
- [ ] Update `train.py`:
  - Set `modality_dropout_prob = 0.2` (20% dropout during training)
  - Disable during validation
- [ ] Test: Train with/without dropout, compare validation AUC

**Acceptance Criteria:**
- ‚úÖ Modality dropout implemented in forward pass
- ‚úÖ Only active during training, disabled during inference
- ‚úÖ Validation AUC improves or stays same

---

## **üìã PHASE 2: HIGH-IMPACT IMPROVEMENTS (Weeks 3‚Äì5)**

### **Priority 2.1: Cross-Modal Attention Fusion**
**Expected Gain:** +2‚Äì5% AUC  
**Effort:** 3‚Äì4 days

**Tasks:**
- [ ] Create `models/fusion.py` with `CrossModalAttentionFusion`:
  ```python
  class CrossModalAttentionFusion(nn.Module):
      def forward(self, video_features, audio_features):
          # video_features: (batch, T, 2048)
          # audio_features: (batch, A, 512)
          # Output: fused features (batch, 2048)
          
          # Cross-attention: video queries, audio keys/values
          attn_weights = softmax(video @ audio.T)
          audio_context = attn_weights @ audio
          fused = concat([video_pooled, audio_context])
          return fused
  ```
- [ ] Update `models/multimodal_model.py`:
  - Replace concatenation with `CrossModalAttentionFusion`
  - Update forward pass: video ‚Üí temporal encoder ‚Üí attention(video, audio) ‚Üí classification
- [ ] Retrain model on FaceForensics++:
  - Check validation AUC improvement
  - Tune attention hidden dimensions
- [ ] Test on Celeb-DF (out-of-distribution)

**Acceptance Criteria:**
- ‚úÖ Cross-attention module implemented
- ‚úÖ Forward pass completes without error
- ‚úÖ Validation AUC improves by 2‚Äì5%
- ‚úÖ Celeb-DF AUC also improves

---

### **Priority 2.2: Optical Flow Features**
**Expected Gain:** +3‚Äì5% AUC  
**Effort:** 4‚Äì5 days

**Tasks:**
- [ ] Create `preprocess/optical_flow.py`:
  - Compute optical flow between adjacent frames using OpenCV (Farneback)
  - Output: flow magnitude + direction maps
  - Cache computed flows to disk
- [ ] Update `data/multimodal_dataset.py`:
  - Load precomputed optical flow for each frame
  - Stack with appearance features: (batch, T, C+2) where +2 is flow
- [ ] Update `models/frame_model.py`:
  - Modify input layer to accept appearance + flow channels
  - Update input shape: (3 + 2, 224, 224) ‚Üí (5, 224, 224)
- [ ] Retrain on FaceForensics++:
  - Check validation AUC improvement
  - Compare with/without optical flow
- [ ] Test generalization on Celeb-DF

**Acceptance Criteria:**
- ‚úÖ Optical flow computed and cached
- ‚úÖ Model accepts 5-channel input (RGB + flow)
- ‚úÖ Validation AUC improves by 3‚Äì5%

---

### **Priority 2.3: Face Alignment**
**Expected Gain:** +1‚Äì2% AUC  
**Effort:** 2‚Äì3 days

**Tasks:**
- [ ] Update `preprocess/face_detection.py`:
  - Use RetinaFace landmarks for alignment (already available)
  - Compute affine transformation to canonical face pose
  - Apply alignment to extracted face crops
- [ ] Update `data/multimodal_dataset.py`:
  - Load aligned face crops instead of bounding box crops
- [ ] Retrain model:
  - Should improve AUC due to pose normalization
  - Expected gain: 1‚Äì2%

**Acceptance Criteria:**
- ‚úÖ Face alignment implemented
- ‚úÖ Aligned crops generated correctly
- ‚úÖ Validation AUC improves by 1‚Äì2%

---

### **Priority 2.4: Uncertainty Estimation**
**Expected Gain:** Explainability (no AUC gain, but reliability)  
**Effort:** 2‚Äì3 days

**Tasks:**
- [ ] Implement MC-Dropout:
  - Enable dropout during inference (10 forward passes)
  - Compute mean + variance of predictions
  - Return confidence intervals
- [ ] Alternative: Temperature Scaling
  - Learn temperature value on validation set
  - Output calibrated probabilities
- [ ] Update output JSON:
  - Add `confidence_interval`: `[lower, upper]`
  - Add `uncertainty_score`: variance / mean
- [ ] Test: Compare MC-Dropout vs Temperature Scaling on Celeb-DF

**Acceptance Criteria:**
- ‚úÖ Uncertainty estimates computed
- ‚úÖ Output includes confidence intervals
- ‚úÖ Calibration improves on held-out data

---

### **Priority 2.5: Multi-Task Learning**
**Expected Gain:** +2‚Äì3% AUC (regularization)  
**Effort:** 3‚Äì4 days

**Tasks:**
- [ ] Create auxiliary task heads in `models/frame_model.py`:
  - Task 1: Facial landmark prediction (68 points)
  - Task 2: Head pose estimation (yaw, pitch, roll)
- [ ] Update training loop:
  - Loss = primary_loss + Œª‚ÇÅ √ó landmark_loss + Œª‚ÇÇ √ó pose_loss
  - Tune Œª‚ÇÅ, Œª‚ÇÇ on validation set (start with 0.1)
- [ ] Retrain model:
  - Should improve generalization
  - Expected gain: 2‚Äì3%

**Acceptance Criteria:**
- ‚úÖ Auxiliary tasks trained jointly
- ‚úÖ Validation AUC improves by 2‚Äì3%
- ‚úÖ Landmark/pose predictions reasonable

---

## **üìã PHASE 3: ADVANCED METHODS (Weeks 6‚Äì9)**

### **Priority 3.1: Transformer-Based Temporal Encoder**
**Expected Gain:** +2‚Äì3% AUC  
**Effort:** 4‚Äì5 days

**Tasks:**
- [ ] Create `models/temporal_transformer.py`:
  - Replace 1D ConvNet with Vision Transformer
  - Input: sequence of 5‚Äì10 frame embeddings (batch, T, 2048)
  - Output: aggregated temporal embedding (batch, 2048)
- [ ] Update `models/multimodal_model.py`:
  - Replace TemporalConv with TemporalTransformer
- [ ] Retrain on FaceForensics++:
  - Check if AUC improves
  - Compare training time (may be slower)

**Acceptance Criteria:**
- ‚úÖ Transformer encoder implemented
- ‚úÖ Validation AUC improves or stays competitive
- ‚úÖ Inference latency acceptable (<60s for 30s video)

---

### **Priority 3.2: Lip-Sync Verification**
**Expected Gain:** +3‚Äì5% AUC  
**Effort:** 5‚Äì6 days

**Tasks:**
- [ ] Create `models/lipsync_detector.py`:
  - Detect lip region in face crop
  - Compute optical flow on lips
  - Compare lip motion frequency with audio speech rate
  - Return lip-sync confidence score
- [ ] Integrate into `models/multimodal_model.py`:
  - Add lip-sync score as auxiliary output
  - Include in final prediction: `confidence = 0.7 √ó deepfake_confidence + 0.3 √ó lipsync_confidence`
- [ ] Retrain:
  - Tune weighting of lip-sync score
  - Expected gain: 3‚Äì5%

**Acceptance Criteria:**
- ‚úÖ Lip-sync detector working
- ‚úÖ Lips detected correctly in face crops
- ‚úÖ Speech rate vs lip motion compared
- ‚úÖ Validation AUC improves by 3‚Äì5%

---

### **Priority 3.3: Ensemble Modeling**
**Expected Gain:** +2‚Äì4% AUC  
**Effort:** 3‚Äì4 days

**Tasks:**
- [ ] Train 5 independent models:
  - Different random seeds
  - Slightly different architectures (dropout rate, learning rate)
  - Same dataset, 10 epochs each
- [ ] Create `serve/ensemble.py`:
  - Load all 5 checkpoints
  - Run inference on all models
  - Average predictions: `ensemble_pred = mean([pred‚ÇÅ, pred‚ÇÇ, ..., pred‚ÇÖ])`
- [ ] Update `/analyze-video` endpoint:
  - Use ensemble instead of single model
  - Return per-model predictions + ensemble average
- [ ] Test on FaceForensics++ + Celeb-DF:
  - Ensemble AUC should be 2‚Äì4% higher than single model

**Acceptance Criteria:**
- ‚úÖ 5 models trained independently
- ‚úÖ Ensemble inference working
- ‚úÖ Validation AUC improves by 2‚Äì4%

---

### **Priority 3.4: Adversarial Robustness**
**Expected Gain:** Robustness (no AUC gain on clean data)  
**Effort:** 3‚Äì4 days

**Tasks:**
- [ ] Create `eval/adversarial_eval.py`:
  - FGSM attacks: Œµ ‚àà {0.01, 0.05, 0.1}
  - PGD attacks: Œ±=0.01, steps=10
  - Test on 100 videos from Celeb-DF
- [ ] Evaluate robustness:
  - How much does AUC drop under attack?
  - Accept <5% AUC drop as good robustness
- [ ] Optional: Adversarial training:
  - Train on mix of clean + FGSM images
  - May improve robustness but reduce clean AUC

**Acceptance Criteria:**
- ‚úÖ Adversarial attacks implemented
- ‚úÖ Robustness evaluated
- ‚úÖ AUC drop <5% under FGSM attack (Œµ=0.05)

---

### **Priority 3.5: Explainability Module**
**Expected Gain:** Interpretability (required for forensics)  
**Effort:** 4‚Äì5 days

**Tasks:**
- [ ] Create `models/explainability.py`:
  - Grad-CAM on final Conv layer of video encoder
  - Feature importance for audio (attention weights)
  - Generate saliency overlay PNG
- [ ] Update `serve/api_video.py`:
  - Compute saliency for top-5 anomalous frames
  - Upload saliency images to Supabase Storage
  - Return saliency URLs in output JSON
- [ ] Frontend update:
  - Display saliency maps on results page
  - Highlight which regions triggered "fake" prediction
  - Show audio anomaly timestamps
- [ ] Test:
  - Verify saliency maps look reasonable
  - User study: do saliency maps help forensic analysts?

**Acceptance Criteria:**
- ‚úÖ Saliency maps generated for anomalous frames
- ‚úÖ Saliency URLs returned in API
- ‚úÖ Frontend displays saliency overlays correctly

---

## **üéØ SUCCESS METRICS**

### **Performance Targets**

| Metric | Current | After Phase 1 | After Phase 2 | After Phase 3 |
|--------|---------|---------------|---------------|---------------|
| **FaceForensics++ AUC** | ~70‚Äì75% | 78‚Äì82% | 83‚Äì87% | 88‚Äì92% |
| **Celeb-DF AUC** | ~65‚Äì70% | 74‚Äì78% | 79‚Äì83% | 84‚Äì88% |
| **Cross-Dataset Generalization** | 60‚Äì65% | 70‚Äì74% | 75‚Äì79% | 80‚Äì84% |
| **False Positive Rate (@ 95% TPR)** | ~10% | ~7% | ~4% | ~2% |
| **Inference Latency (30s video)** | N/A | 45‚Äì60s | 50‚Äì65s | 50‚Äì70s |

### **Quality Metrics**

| Criterion | Target |
|-----------|--------|
| **Model Explainability** | ‚úÖ Saliency maps + artifact explanations |
| **Confidence Calibration** | ‚úÖ Expected Calibration Error < 0.05 |
| **Adversarial Robustness** | ‚úÖ AUC drop < 5% under FGSM (Œµ=0.05) |
| **Cross-Codec Robustness** | ‚úÖ AUC within 2% for H.264 + H.265 |
| **Code Quality** | ‚úÖ Type hints, docstrings, unit tests |

---

## **üìÖ TIMELINE**

| Phase | Duration | Start | End | Key Deliverables |
|-------|----------|-------|-----|------------------|
| **Phase 1** | 2 weeks | Jan 6 | Jan 20 | Functional multimodal model + async API |
| **Phase 2** | 3 weeks | Jan 21 | Feb 10 | Attention fusion + optical flow + explainability |
| **Phase 3** | 4 weeks | Feb 11 | Mar 10 | Transformer + lip-sync + ensemble + robustness |
| **Testing & Deployment** | 1 week | Mar 11 | Mar 17 | Final validation + production deployment |

**Total:** 12 weeks (Q1 2026)

---

## **üîß DEVELOPMENT GUIDELINES**

### **Code Organization**

```
model-service/src/
  models/
    ‚îú‚îÄ‚îÄ frame_model.py (EfficientNet-B3 + multi-task heads)
    ‚îú‚îÄ‚îÄ audio_encoder.py (wav2vec2 wrapper) ‚Üê NEW
    ‚îú‚îÄ‚îÄ multimodal_model.py (fusion + classification)
    ‚îú‚îÄ‚îÄ fusion.py (CrossModalAttentionFusion) ‚Üê NEW
    ‚îú‚îÄ‚îÄ temporal_transformer.py (Transformer temporal encoder) ‚Üê NEW
    ‚îú‚îÄ‚îÄ lipsync_detector.py (lip-sync verification) ‚Üê NEW
    ‚îú‚îÄ‚îÄ explainability.py (Grad-CAM + feature importance) ‚Üê NEW
    ‚îî‚îÄ‚îÄ losses.py (temporal consistency + multi-task losses)
  
  data/
    ‚îú‚îÄ‚îÄ multimodal_dataset.py (loader with VAD)
    ‚îî‚îÄ‚îÄ augmentation.py (audio + video augmentation)
  
  preprocess/
    ‚îú‚îÄ‚îÄ extract_frames.py (existing)
    ‚îú‚îÄ‚îÄ extract_audio.py (NEW)
    ‚îú‚îÄ‚îÄ voice_activity_detection.py (NEW)
    ‚îú‚îÄ‚îÄ optical_flow.py (NEW)
    ‚îî‚îÄ‚îÄ face_detection.py (with alignment)
  
  serve/
    ‚îú‚îÄ‚îÄ api.py (frame-level inference)
    ‚îú‚îÄ‚îÄ api_video.py (video-level async inference) ‚Üê NEW
    ‚îú‚îÄ‚îÄ inference.py (aggregation + saliency) ‚Üê NEW
    ‚îî‚îÄ‚îÄ ensemble.py (ensemble inference) ‚Üê NEW
  
  train.py (updated with new losses + tasks)
  eval/
    ‚îî‚îÄ‚îÄ multimodal_eval.py (comprehensive metrics)
```

### **Testing Strategy**

- **Unit Tests:** Each new module (audio_encoder, fusion, explainability)
- **Integration Tests:** End-to-end video inference pipeline
- **Regression Tests:** AUC on FaceForensics++ + Celeb-DF after each phase
- **Robustness Tests:** Adversarial attacks, codec variations, resolution changes

### **Code Review Checklist**

- [ ] Type hints for all function signatures
- [ ] Docstrings explaining algorithm + parameters
- [ ] Logging statements for debugging
- [ ] Unit tests with >80% coverage
- [ ] Backward compatibility with existing checkpoints (if applicable)

---

## **üìù DEPENDENCIES & REQUIREMENTS**

### **New Python Packages**

```
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.30.0 (for wav2vec2)
librosa>=0.10.0 (VAD, audio processing)
opencv-python>=4.8.0 (optical flow)
pyannote-audio (optional, for better VAD)
celery>=5.3.0 (async job queue)
redis>=4.5.0 (Celery broker)
```

### **Pre-Trained Model Downloads**

```bash
# Wav2vec2 (will download on first use)
from transformers import Wav2Vec2Model
Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")

# Face detection (if not already cached)
pip install retinaface-pytorch
```

---

## **‚ùì BLOCKERS & RISKS**

### **Known Risks**

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|-----------|
| **GPU Memory** (all phases combined) | Medium | High | Gradient checkpointing, smaller batch size |
| **Audio encoder slow** (wav2vec2 inference) | Low | Medium | Quantize encoder, use smaller model |
| **Cross-dataset AUC drop** | Medium | High | Early stopping on Celeb-DF, data augmentation |
| **Explainability overhead** | Low | Medium | Compute saliency asynchronously |

### **Assumptions**

- ‚úÖ FaceForensics++ + Celeb-DF available for training
- ‚úÖ Sufficient GPU memory (RTX 3090 / A100)
- ‚úÖ Celery + Redis available for async job queue
- ‚úÖ Supabase storage available for saliency uploads

---

## **‚úÖ SIGN-OFF & NEXT STEPS**

**Document Status:** Ready for implementation  
**Model Contract Locked:** Yes (see ML_SYSTEM_DESIGN.md ¬ß XI)  
**Expected Completion:** March 17, 2026

**Next Action:** Start Phase 1.1 (Audio Encoder Replacement)
