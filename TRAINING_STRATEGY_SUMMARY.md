# **TRAINING & EVALUATION STRATEGY â€” INTEGRATION SUMMARY**

**New Document:** [TRAINING_EVALUATION_STRATEGY.md](TRAINING_EVALUATION_STRATEGY.md) (36KB)  
**Status:** âœ… PLAN MODE COMPLETE  
**Date:** January 3, 2026

---

## **ðŸ“ HOW THIS FITS IN**

**Document Hierarchy:**

```
MODEL_CONTRACT_v1.md (Locked decisions)
        â†“
TRAINING_EVALUATION_STRATEGY.md (NEW)
        â†“
IMPLEMENTATION_ROADMAP.md (Phase 1â€“3 code)
        â†“
CODE_IMPACT_ANALYSIS.md (Specific implementation)
```

---

## **ðŸ“Š WHAT'S IN THE STRATEGY DOCUMENT**

### **I. Dataset Strategy** (Section I)
- âœ… Primary dataset: FaceForensics++ (265 train, 33 val, 33 test)
- âœ… Secondary dataset: Celeb-DF v2 (hold-out cross-dataset eval)
- âœ… Augmentation: DFDC (Phase 3 optional)
- âœ… Class imbalance handling (balanced datasets for v1)
- âœ… Real vs Fake diversity matrix (4 generators Ã— compression levels)

### **II. Label Granularity** (Section II)
- âœ… Video-level labels (primary, unambiguous)
- âœ… Frame-level pseudo-labels (derived, hard assignment)
- âœ… Segment-level supervision (1-sec windows)
- âœ… Weak vs strong labels (hybrid strategy)

### **III. Training Pipeline** (Section III)
- âœ… Per-modality pretraining (video + audio)
- âœ… **Staged training (3 stages, RECOMMENDED for v1):**
  - Stage 1: Video backbone + temporal encoder (5 epochs)
  - Stage 2: Audio encoder fine-tuning (5 epochs)
  - Stage 3: Joint fusion training (10 epochs)
- âœ… Freezing schedule (backbone frozen, temporal/audio trainable)
- âœ… Augmentations (conservative video + audio, Phase 2 expansion)

### **IV. Loss Functions** (Section IV)
- âœ… Primary: Binary cross-entropy with label smoothing
- âœ… Temporal consistency loss (Î»=0.1, penalizes frame variance)
- âœ… Video-level aggregation loss (ensure video-accuracy)
- âœ… Modality disagreement (Phase 2, Î»=0.05)
- âœ… Confidence calibration (Phase 2, focal loss)
- âœ… **Total loss: L = 1.0Ã—L_bce + 0.1Ã—L_video + 0.1Ã—L_temporal**

### **V. Evaluation Protocol** (Section V)
- âœ… Primary metrics: AUC, EER, Average Precision
- âœ… Secondary metrics: Accuracy, Precision, Recall, F1, ECE
- âœ… **Cross-dataset generalization (FF++ â†’ Celeb-DF)**
- âœ… Modality ablation tests (video-only vs audio-only vs joint)
- âœ… Failure case analysis (per-generator, per-compression, per-modality)

### **VI. Overfitting & Risks** (Section VI)
- âœ… Dataset bias risks (generator overfitting, compression bias, demographic bias)
- âœ… Modality-specific leakage (audio artifacts, compression shortcuts)
- âœ… Temporal leakage (generator-specific jitter patterns)
- âœ… Confidence calibration shortcuts (overconfidence on in-distribution)

### **VII. Training Checklist** (Section VII)
- âœ… Pre-training setup (data prep, model architecture, optimization)
- âœ… Training validation checkpoints (per-epoch monitoring)
- âœ… Final evaluation protocol (comprehensive testing)

### **VIII. 15 Clarification Questions** (Section VIII)
- **Q1â€“3:** Data availability (FaceForensics++, Celeb-DF, DFDC)
- **Q4â€“6:** Labeling & annotation (frame-level, generator metadata)
- **Q7â€“9:** Training decisions (staged vs joint, fine-tuning depth)
- **Q10â€“11:** Augmentation strategy (compression levels, audio aggressiveness)
- **Q12â€“15:** Evaluation & reporting (per-generator, cross-codec, explainability)

### **IX. Expected Outcomes** (Section IX)
- âœ… V1 baseline performance prediction
  - FaceForensics++ test: 0.84â€“0.87 AUC âœ…
  - Celeb-DF test: 0.78â€“0.82 AUC âœ…
  - Generalization gap: ~0.05â€“0.08 (acceptable)

---

## **ðŸ”‘ KEY DESIGN DECISIONS**

### **Training Approach: STAGED (3-Stage)**

**Why staged over end-to-end:**
- âœ… Modular debugging (isolate video/audio issues)
- âœ… Lower memory during early stages
- âœ… Clear convergence checkpoints
- âœ… Each modality optimized separately before fusion
- âš ï¸ Trade-off: 1.5Ã— longer training time (acceptable for v1)

**Stage breakdown:**
- Stage 1 (5 epochs): Video-only with frozen backbone
- Stage 2 (5 epochs): Audio-only with frozen backbone  
- Stage 3 (10 epochs): Joint fusion with selective unfreezing

---

### **Loss Function: Multi-Objective**

**Why 3 terms instead of single cross-entropy:**
- **L_bce (segment):** Direct classification signal (segment-level)
- **L_video:** Ensure video-level accuracy (coarse constraint)
- **L_temporal:** Regularization for smooth representations (generalization)

**Total loss:** L = 1.0Ã—L_bce + 0.1Ã—L_video + 0.1Ã—L_temporal

---

### **Dataset Split: Stratified by Generator**

**Why stratification matters:**
- Ensures each split (train/val/test) has similar generator distribution
- Prevents: All DeepFaceLab in train, all Face2Face in test (would falsely inflate AUC)
- Recommendation: Stratify by generator when splitting FaceForensics++

---

### **Evaluation: Mandatory Cross-Dataset**

**FaceForensics++ test AUC alone is insufficient:**
- âœ… Test on Celeb-DF (different generator, compression, speaker dist)
- âœ… Report generalization gap (gap >10% = overfitting)
- âœ… Celeb-DF is primary metric for final v1.0 sign-off

---

## **âš ï¸ CRITICAL RISKS IDENTIFIED**

| Risk | Mitigation | Monitoring |
|------|-----------|-----------|
| **Generator overfitting** | Stratified split, per-generator AUC | AUC variance >15% = FLAG |
| **Compression artifacts** | Augmentation Phase 2, c23 baseline | Test c0, c40 separately |
| **Audio-only leakage** | VAD ensures realistic audio, FaceForensics++ sourced | Video-only ablation AUC |
| **Temporal shortcuts** | Cross-dataset test (Celeb-DF uses Wav2Lip) | Celeb-DF AUC gap >10% = FLAG |
| **Overconfidence** | Temperature scaling, calibration metric ECE | ECE > 0.05 = recalibrate |

---

## **âœ… RECOMMENDED PRE-TRAINING CHECKLIST**

**Before running Stage 1:**

- [ ] FaceForensics++ downloaded & preprocessed
- [ ] Frames extracted @ 5 FPS, faces detected
- [ ] Audio extracted, VAD applied
- [ ] Train/val/test splits created (stratified by generator)
- [ ] Dataset statistics computed & validated
- [ ] EfficientNet-B3 loaded (ImageNet pretrained)
- [ ] wav2vec2-base loaded (speech pretrained)
- [ ] Model architecture verified (parameter counts match)
- [ ] Optimization hyperparameters set (AdamW, learning rate schedule)
- [ ] Loss functions implemented (3-term loss)
- [ ] Early stopping configured (patience=3, min_delta=0.002)

---

## **ðŸ“‹ 15 CLARIFICATION QUESTIONS SUMMARY**

**These need to be answered before implementation:**

1. **Data:** Do we have FaceForensics++ full access?
2. **Data:** Is Celeb-DF audio real or synthetic TTS?
3. **Data:** Budget for DFDC (Phase 3)?
4. **Labels:** Any frame-level deepfake confidence scores available?
5. **Labels:** Does FF++ metadata specify generator per video?
6. **Labels:** Is FaceForensics++ audio original or replaced?
7. **Training:** Preference for staged (3-stage) vs joint (end-to-end)?
8. **Training:** Audio encoder fine-tune depth (1 block vs 4 blocks)?
9. **Training:** Is 1-second temporal window flexible?
10. **Augmentation:** Train on c23 only or mix c0/c23/c40?
11. **Augmentation:** Audio augmentation aggressiveness level?
12. **Evaluation:** Per-generator AUC reporting (mandatory)?
13. **Evaluation:** Cross-codec testing (H.265 optional)?
14. **Evaluation:** Saliency maps during validation (Phase 3)?
15. **Evaluation:** Confidence threshold strategy (0.5 vs optimized)?

**If you have answers to these, we can finalize the training protocol.**

---

## **ðŸš€ NEXT STEPS**

1. **Review:** Read [TRAINING_EVALUATION_STRATEGY.md](TRAINING_EVALUATION_STRATEGY.md) (20 min)
2. **Clarify:** Answer the 15 questions in Section VIII (10 min)
3. **Confirm:** Validate strategy aligns with available data/resources (10 min)
4. **Proceed:** Finalize training protocol, begin data preprocessing (Phase 1)

---

**Document Status:** âœ… PLAN MODE COMPLETE  
**Ready for Data Preprocessing:** YES  
**Ready for Implementation:** AFTER answering Q1â€“Q15

**Total Design Freeze Documentation:** 10 files, ~200KB
